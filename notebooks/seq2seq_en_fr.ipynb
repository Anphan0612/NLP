{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq LSTM English→French (Multi30K)\n",
    "\n",
    "- Encoder-Decoder LSTM with fixed context vector, teacher forcing, greedy decoding.\n",
    "- Optional Luong attention + beam search for extended experiments.\n",
    "- Dataset: Multi30K en-fr raw (train/val/test) downloaded from GitHub.\n",
    "- Outputs: `best_model.pth`, `translate(sentence)`, BLEU score, loss plots, 5 example translations + error analysis notes.\n",
    "\n",
    "> Run this notebook end-to-end on Colab GPU (Python 3, torch ≥1.13).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab-friendly). Run this cell first!\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run(cmd):\n",
    "    print(cmd)\n",
    "    subprocess.check_call(cmd, shell=True)\n",
    "\n",
    "# Install PyTorch + torchtext (CUDA 12.1 compatible)\n",
    "run(\"pip install -U --no-cache-dir torch==2.4.0 torchvision==0.19.0 torchtext==0.18.0 --extra-index-url https://download.pytorch.org/whl/cu121\")\n",
    "# Fallback if above fails: run(\"pip install -U torch torchvision torchtext\")\n",
    "\n",
    "# Install other dependencies\n",
    "run(\"pip install -U spacy nltk tqdm matplotlib requests\")\n",
    "\n",
    "# Download spaCy models\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"data/multi30k_en_fr\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "for p in [RAW_DIR, PROCESSED_DIR, CHECKPOINT_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download Multi30K en-fr raw files if missing\n",
    "# Using direct GitHub raw URLs (if 404, files may need to be cloned from repo)\n",
    "base_url = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
    "urls = {\n",
    "    \"train.en\": base_url + \"train.en\",\n",
    "    \"train.fr\": base_url + \"train.fr\",\n",
    "    \"val.en\": base_url + \"val.en\",\n",
    "    \"val.fr\": base_url + \"val.fr\",\n",
    "    \"test.en\": base_url + \"test_2016_flickr.en\",\n",
    "    \"test.fr\": base_url + \"test_2016_flickr.fr\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "def download_file(url, dest):\n",
    "    if dest.exists():\n",
    "        print(f\"✓ {dest.name} already exists\")\n",
    "        return True\n",
    "    print(f\"Downloading {dest.name} ...\")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        dest.write_bytes(r.content)\n",
    "        print(f\"✓ Downloaded {dest.name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to download {dest.name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Try downloading from GitHub raw\n",
    "all_success = True\n",
    "for fname, url in urls.items():\n",
    "    if not download_file(url, RAW_DIR / fname):\n",
    "        all_success = False\n",
    "\n",
    "# Fallback: clone repo if direct download fails\n",
    "if not all_success:\n",
    "    print(\"\\nDirect download failed. Cloning repository (this may take a few minutes)...\")\n",
    "    clone_dir = DATA_DIR.parent / \"multi30k-dataset\"\n",
    "    \n",
    "    # Remove incomplete clone if exists\n",
    "    if clone_dir.exists():\n",
    "        if not (clone_dir / \"data\" / \"task1\" / \"raw\" / \"train.en\").exists():\n",
    "            print(\"Removing incomplete clone...\")\n",
    "            import shutil\n",
    "            shutil.rmtree(clone_dir, ignore_errors=True)\n",
    "    \n",
    "    # Clone if not exists or was removed\n",
    "    if not clone_dir.exists() or not (clone_dir / \"data\" / \"task1\" / \"raw\" / \"train.en\").exists():\n",
    "        print(\"Cloning multi30k/dataset repository...\")\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", \"--recursive\", \"https://github.com/multi30k/dataset.git\", str(clone_dir)],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Git clone error: {result.stderr}\")\n",
    "            print(\"Trying to check if files exist in different location...\")\n",
    "        else:\n",
    "            print(\"✓ Repository cloned successfully\")\n",
    "    \n",
    "    # Check and initialize submodules if needed\n",
    "    if (clone_dir / \".gitmodules\").exists():\n",
    "        print(\"Initializing git submodules...\")\n",
    "        subprocess.run([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], \n",
    "                      cwd=str(clone_dir), timeout=300, capture_output=True)\n",
    "    \n",
    "    # Search for files recursively\n",
    "    import shutil\n",
    "    import os\n",
    "    \n",
    "    def find_files_recursive(directory, pattern):\n",
    "        \"\"\"Find all files matching pattern recursively\"\"\"\n",
    "        found = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file == pattern:\n",
    "                    found.append(Path(root) / file)\n",
    "        return found\n",
    "    \n",
    "    print(\"Searching for data files...\")\n",
    "    \n",
    "    # Map of what we need vs what might be available\n",
    "    file_mapping = {\n",
    "        \"train.en\": [\"train.en\", \"train.lc.norm.tok.en\"],\n",
    "        \"train.fr\": [\"train.fr\", \"train.lc.norm.tok.fr\"],\n",
    "        \"val.en\": [\"val.en\", \"val.lc.norm.tok.en\"],\n",
    "        \"val.fr\": [\"val.fr\", \"val.lc.norm.tok.fr\"],\n",
    "        \"test.en\": [\"test_2016_flickr.en\", \"test_2016_flickr.lc.norm.tok.en\"],\n",
    "        \"test.fr\": [\"test_2016_flickr.fr\", \"test_2016_flickr.lc.norm.tok.fr\"],\n",
    "    }\n",
    "    \n",
    "    all_found = True\n",
    "    for target_name, possible_names in file_mapping.items():\n",
    "        found = False\n",
    "        for possible_name in possible_names:\n",
    "            found_files = find_files_recursive(clone_dir, possible_name)\n",
    "            if found_files:\n",
    "                src = found_files[0]\n",
    "                dest = RAW_DIR / target_name\n",
    "                if not dest.exists():\n",
    "                    # If it's a tokenized file, we'll use it as-is (spaCy will retokenize anyway)\n",
    "                    shutil.copy2(src, dest)\n",
    "                    print(f\"✓ Copied {target_name} from {src.name} (location: {src.parent})\")\n",
    "                else:\n",
    "                    print(f\"✓ {target_name} already exists\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"✗ {target_name} not found (searched: {possible_names})\")\n",
    "            all_found = False\n",
    "    \n",
    "    if not all_found:\n",
    "        print(f\"\\nNote: Some raw files not found. Tokenized files may be used instead.\")\n",
    "        print(f\"Listing available files in data/task1/...\")\n",
    "        task1_dir = clone_dir / \"data\" / \"task1\"\n",
    "        if task1_dir.exists():\n",
    "            for subdir in [\"raw\", \"tok\"]:\n",
    "                subdir_path = task1_dir / subdir\n",
    "                if subdir_path.exists():\n",
    "                    print(f\"\\n{subdir}/:\")\n",
    "                    for item in sorted(subdir_path.glob(\"*\")):\n",
    "                        if item.is_file():\n",
    "                            print(f\"  {item.name}\")\n",
    "\n",
    "print(\"Data ready at\", RAW_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy tokenizers\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text: str) -> List[str]:\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text.strip())]\n",
    "\n",
    "\n",
    "def tokenize_fr(text: str) -> List[str]:\n",
    "    return [tok.text.lower() for tok in spacy_fr.tokenizer(text.strip())]\n",
    "\n",
    "SPECIALS = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parallel(split: str) -> Tuple[List[str], List[str]]:\n",
    "    en_path = RAW_DIR / f\"{split}.en\"\n",
    "    fr_path = RAW_DIR / f\"{split}.fr\"\n",
    "    with en_path.open(\"r\", encoding=\"utf-8\") as f_en, fr_path.open(\"r\", encoding=\"utf-8\") as f_fr:\n",
    "        en_lines = [line.strip() for line in f_en]\n",
    "        fr_lines = [line.strip() for line in f_fr]\n",
    "    assert len(en_lines) == len(fr_lines), \"Mismatched parallel data\"\n",
    "    return en_lines, fr_lines\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens, specials, max_size=10000, min_freq=2):\n",
    "        self.specials = specials\n",
    "        self.stoi = {spec: idx for idx, spec in enumerate(specials)}\n",
    "        self.itos = specials.copy()\n",
    "        counter = Counter()\n",
    "        for token_list in tokens:\n",
    "            counter.update(token_list)\n",
    "        for token, count in counter.most_common(max_size - len(specials)):\n",
    "            if count >= min_freq and token not in self.stoi:\n",
    "                self.stoi[token] = len(self.itos)\n",
    "                self.itos.append(token)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, UNK_IDX)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def get_itos(self):\n",
    "        return self.itos\n",
    "\n",
    "\n",
    "def build_vocabs(max_size=10000, min_freq=2):\n",
    "    train_en, train_fr = read_parallel(\"train\")\n",
    "    en_tokens = [tokenize_en(line) for line in train_en]\n",
    "    fr_tokens = [tokenize_fr(line) for line in train_fr]\n",
    "    en_vocab = Vocab(en_tokens, SPECIALS, max_size=max_size, min_freq=min_freq)\n",
    "    fr_vocab = Vocab(fr_tokens, SPECIALS, max_size=max_size, min_freq=min_freq)\n",
    "    return en_vocab, fr_vocab\n",
    "\n",
    "\n",
    "en_vocab, fr_vocab = build_vocabs()\n",
    "print(\"Vocab sizes:\", len(en_vocab), len(fr_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTextDataset(Dataset):\n",
    "    def __init__(self, split: str):\n",
    "        self.en_lines, self.fr_lines = read_parallel(split)\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_tok = tokenize_en(self.en_lines[idx])\n",
    "        fr_tok = tokenize_fr(self.fr_lines[idx])\n",
    "        en_ids = [SOS_IDX] + [en_vocab[t] for t in en_tok] + [EOS_IDX]\n",
    "        fr_ids = [SOS_IDX] + [fr_vocab[t] for t in fr_tok] + [EOS_IDX]\n",
    "        return torch.tensor(en_ids, dtype=torch.long), torch.tensor(fr_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    # sort by src length desc for packing\n",
    "    src_batch = list(src_batch)\n",
    "    tgt_batch = list(tgt_batch)\n",
    "    lengths = torch.tensor([len(x) for x in src_batch])\n",
    "    sorted_idx = torch.argsort(lengths, descending=True)\n",
    "    src_batch = [src_batch[i] for i in sorted_idx]\n",
    "    tgt_batch = [tgt_batch[i] for i in sorted_idx]\n",
    "    src_padded = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_padded = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_padded.to(device), tgt_padded.to(device), lengths[sorted_idx].to(device)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_ds = ParallelTextDataset(\"train\")\n",
    "val_ds = ParallelTextDataset(\"val\")\n",
    "test_ds = ParallelTextDataset(\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Batches:\", len(train_loader), len(val_loader), len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        # src: [src_len, batch]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        packed = pack_padded_sequence(embedded, src_lengths.cpu(), enforce_sorted=True)\n",
    "        outputs, (hidden, cell) = self.rnn(packed)\n",
    "        outputs, _ = pad_packed_sequence(outputs)  # [src_len, batch, hid_dim * num_directions]\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim, hid_dim, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: [batch, hid_dim]; encoder_outputs: [src_len, batch, hid_dim]\n",
    "        scores = torch.einsum(\"bh,sbh->bs\", self.attn(hidden), encoder_outputs)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # [batch, src_len]\n",
    "        context = torch.einsum(\"bs,sbh->bh\", attn_weights, encoder_outputs)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim * (2 if use_attention else 1), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.use_attention = use_attention\n",
    "        self.attention = LuongAttention(hid_dim) if use_attention else None\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs=None, mask=None):\n",
    "        # input: [batch]; hidden/cell: [n_layers, batch, hid_dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch, emb_dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output: [1, batch, hid_dim]\n",
    "        output = output.squeeze(0)\n",
    "        if self.use_attention:\n",
    "            context, attn_weights = self.attention(output, encoder_outputs, mask)\n",
    "            output_cat = torch.cat((output, context), dim=1)\n",
    "            prediction = self.fc_out(output_cat)\n",
    "        else:\n",
    "            prediction = self.fc_out(output)\n",
    "            attn_weights = None\n",
    "        return prediction, hidden, cell, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx=PAD_IDX, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "    def make_mask(self, src):\n",
    "        # src: [src_len, batch]\n",
    "        return (src != self.pad_idx).permute(1, 0)  # [batch, src_len]\n",
    "\n",
    "    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [src_len, batch]; trg: [trg_len, batch]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size, device=device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
    "        input = trg[0, :]  # first token = <sos>\n",
    "        mask = self.make_mask(src) if self.use_attention else None\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and model init\n",
    "# BẢN GỐC (khớp 100% với đề bài):\n",
    "EMB_DIM = 300\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3  # Đề bài: 0.3-0.5, chọn 0.3\n",
    "USE_ATTENTION = True  # set False to train baseline without attention\n",
    "BEAM_SIZE = 5\n",
    "MAX_LEN = 50\n",
    "TEACHER_FORCING = 0.5  # Đề bài: 0.5 cố định\n",
    "LR = 0.001  # Đề bài: Adam(lr=0.001)\n",
    "EPOCHS = 15  # Đề bài: 10-20\n",
    "PATIENCE = 3  # Đề bài: dừng nếu val_loss không giảm sau 3 epoch\n",
    "\n",
    "# BẢN CẢI TIẾN (để so sánh - comment lại):\n",
    "# DROPOUT = 0.45  # Tăng để giảm overfitting\n",
    "# TEACHER_FORCING_START = 0.7  # Scheduled teacher forcing\n",
    "# TEACHER_FORCING_END = 0.3\n",
    "# LABEL_SMOOTHING = 0.1  # Label smoothing\n",
    "# LR = 8e-4  # Learning rate thấp hơn\n",
    "# PATIENCE = 4  # Patience cao hơn\n",
    "\n",
    "enc = Encoder(len(en_vocab), EMB_DIM, HID_DIM, n_layers=N_LAYERS, dropout=DROPOUT)\n",
    "dec = Decoder(len(fr_vocab), EMB_DIM, HID_DIM, n_layers=N_LAYERS, dropout=DROPOUT, use_attention=USE_ATTENTION)\n",
    "model = Seq2Seq(enc, dec, pad_idx=PAD_IDX, use_attention=USE_ATTENTION).to(device)\n",
    "\n",
    "# BẢN GỐC (khớp đề bài):\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # Đề bài: không có label_smoothing\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)  # Đề bài: Adam(lr=0.001), không có weight_decay\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)  # Đề bài: tùy chọn\n",
    "\n",
    "# BẢN CẢI TIẾN (để so sánh - comment lại):\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start, end):\n",
    "    elapsed = end - start\n",
    "    return int(elapsed // 60), int(elapsed % 60)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, teacher_forcing=0.5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg, src_lengths in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, src_lengths, trg, teacher_forcing_ratio=teacher_forcing)\n",
    "        # outputs: [trg_len, batch, vocab]\n",
    "        output_dim = outputs.shape[-1]\n",
    "        outputs_flat = outputs[1:].reshape(-1, output_dim)\n",
    "        trg_flat = trg[1:].reshape(-1)\n",
    "        loss = criterion(outputs_flat, trg_flat)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg, src_lengths in tqdm(loader, desc=\"Val\", leave=False):\n",
    "            outputs = model(src, src_lengths, trg, teacher_forcing_ratio=0.0)\n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs_flat = outputs[1:].reshape(-1, output_dim)\n",
    "            trg_flat = trg[1:].reshape(-1)\n",
    "            loss = criterion(outputs_flat, trg_flat)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "patience_counter = 0\n",
    "train_losses, val_losses = [], []\n",
    "best_path = CHECKPOINT_DIR / \"best_model.pth\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    # BẢN GỐC (khớp đề bài): Teacher forcing 0.5 cố định\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, teacher_forcing=TEACHER_FORCING)\n",
    "    \n",
    "    # BẢN CẢI TIẾN (để so sánh - comment lại):\n",
    "    # Scheduled teacher forcing: decay linearly from START to END\n",
    "    # current_tf = TEACHER_FORCING_START - (TEACHER_FORCING_START - TEACHER_FORCING_END) * (epoch - 1) / (EPOCHS - 1)\n",
    "    # current_tf = max(TEACHER_FORCING_END, current_tf)\n",
    "    # train_loss = train_one_epoch(model, train_loader, optimizer, criterion, teacher_forcing=current_tf)\n",
    "    \n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    scheduler.step(val_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\"model_state\": model.state_dict(), \"config\": {\n",
    "            \"EMB_DIM\": EMB_DIM,\n",
    "            \"HID_DIM\": HID_DIM,\n",
    "            \"N_LAYERS\": N_LAYERS,\n",
    "            \"DROPOUT\": DROPOUT,\n",
    "            \"USE_ATTENTION\": USE_ATTENTION,\n",
    "            \"EN_VOCAB\": len(en_vocab),\n",
    "            \"FR_VOCAB\": len(fr_vocab),\n",
    "            \"PAD_IDX\": PAD_IDX,\n",
    "        }}, best_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    mins, secs = epoch_time(start_time, time.time())\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train {train_loss:.3f} | Val {val_loss:.3f} | TF {TEACHER_FORCING} | Time {mins}m {secs}s\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "print(\"Best checkpoint:\", best_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train/Val Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference helpers\n",
    "\n",
    "def ids_to_sentence(ids, vocab):\n",
    "    tokens = []\n",
    "    for i in ids:\n",
    "        if i == EOS_IDX:\n",
    "            break\n",
    "        if i not in (SOS_IDX, PAD_IDX):\n",
    "            tokens.append(vocab.get_itos()[i])\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def encode_sentence(sentence, tokenizer, vocab):\n",
    "    toks = [t for t in tokenizer(sentence)]\n",
    "    ids = [SOS_IDX] + [vocab[t] for t in toks] + [EOS_IDX]\n",
    "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "def greedy_decode(model, sentence, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_tensor = encode_sentence(sentence, tokenize_en, en_vocab).unsqueeze(1)\n",
    "        src_len = torch.tensor([src_tensor.shape[0]], device=device)\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)\n",
    "        mask = model.make_mask(src_tensor) if model.use_attention else None\n",
    "        input_token = torch.tensor([SOS_IDX], device=device)\n",
    "        preds = []\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell, _ = model.decoder(input_token, hidden, cell, encoder_outputs, mask)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == EOS_IDX:\n",
    "                break\n",
    "            preds.append(top1.item())\n",
    "            input_token = top1\n",
    "        return ids_to_sentence(preds, fr_vocab)\n",
    "\n",
    "\n",
    "def beam_search_decode(model, sentence, beam_size=5, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_tensor = encode_sentence(sentence, tokenize_en, en_vocab).unsqueeze(1)\n",
    "        src_len = torch.tensor([src_tensor.shape[0]], device=device)\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)\n",
    "        mask = model.make_mask(src_tensor) if model.use_attention else None\n",
    "\n",
    "        beams = [(0.0, [SOS_IDX], hidden, cell)]\n",
    "        completed = []\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for log_prob, seq, h, c in beams:\n",
    "                inp = torch.tensor([seq[-1]], device=device)\n",
    "                output, h_new, c_new, _ = model.decoder(inp, h, c, encoder_outputs, mask)\n",
    "                probs = torch.log_softmax(output, dim=1)\n",
    "                topk_logp, topk_idx = probs.topk(beam_size, dim=1)\n",
    "                for k in range(beam_size):\n",
    "                    next_token = topk_idx[0, k].item()\n",
    "                    new_log_prob = log_prob + topk_logp[0, k].item()\n",
    "                    new_seq = seq + [next_token]\n",
    "                    if next_token == EOS_IDX:\n",
    "                        completed.append((new_log_prob, new_seq))\n",
    "                    else:\n",
    "                        new_beams.append((new_log_prob, new_seq, h_new, c_new))\n",
    "            beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "            if not beams:\n",
    "                break\n",
    "        if completed:\n",
    "            best = max(completed, key=lambda x: x[0])\n",
    "        else:\n",
    "            best = max(beams, key=lambda x: x[0])\n",
    "        return ids_to_sentence(best[1], fr_vocab)\n",
    "\n",
    "\n",
    "def translate(sentence, use_beam=False):\n",
    "    if use_beam:\n",
    "        return beam_search_decode(model, sentence, beam_size=BEAM_SIZE, max_len=MAX_LEN)\n",
    "    return greedy_decode(model, sentence, max_len=MAX_LEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU evaluation on test set\n",
    "\n",
    "def compute_bleu(model, loader, n_samples=None, use_beam=False):\n",
    "    model.eval()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg, src_lengths) in enumerate(tqdm(loader, desc=\"Test\", leave=False)):\n",
    "            # decode each sentence individually for simplicity\n",
    "            for b in range(src.shape[1]):\n",
    "                src_sent = src[:, b].tolist()\n",
    "                trg_sent = trg[:, b].tolist()\n",
    "                # reconstruct raw English sentence for translation\n",
    "                en_tokens = [en_vocab.get_itos()[idx] for idx in src_sent if idx not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
    "                src_text = \" \".join(en_tokens)\n",
    "                pred = translate(src_text, use_beam=use_beam)\n",
    "                ref_tokens = [fr_vocab.get_itos()[idx] for idx in trg_sent if idx not in (PAD_IDX, SOS_IDX, EOS_IDX, EOS_IDX)]\n",
    "                scores.append(sentence_bleu([ref_tokens], pred.split(), smoothing_function=smoothie))\n",
    "            if n_samples and len(scores) >= n_samples:\n",
    "                break\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# Compute BLEU (may take time). Uncomment to run after training.\n",
    "print(\"Computing BLEU scores...\")\n",
    "bleu_greedy = compute_bleu(model, test_loader, n_samples=200, use_beam=False)\n",
    "print(f\"BLEU (greedy): {bleu_greedy:.4f}\")\n",
    "\n",
    "# Compare with beam search (optional, for extension)\n",
    "bleu_beam = compute_bleu(model, test_loader, n_samples=200, use_beam=True)\n",
    "print(f\"BLEU (beam search, size={BEAM_SIZE}): {bleu_beam:.4f}\")\n",
    "print(f\"Improvement: +{(bleu_beam - bleu_greedy)*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 ví dụ dịch + phân tích lỗi (yêu cầu báo cáo)\n",
    "sentences = [\n",
    "    \"a man is riding a bicycle\",\n",
    "    \"children are playing in the park\",\n",
    "    \"a woman is sitting at a table\",\n",
    "    \"two dogs are running on the beach\",\n",
    "    \"the man holds a red umbrella\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"5 VÍ DỤ DỊCH + PHÂN TÍCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "examples = []\n",
    "for i, s in enumerate(sentences, 1):\n",
    "    pred_greedy = translate(s, use_beam=False)\n",
    "    pred_beam = translate(s, use_beam=True)\n",
    "    \n",
    "    print(f\"\\n[{i}] EN: {s}\")\n",
    "    print(f\"    FR (greedy): {pred_greedy}\")\n",
    "    print(f\"    FR (beam):   {pred_beam}\")\n",
    "    \n",
    "    examples.append({\n",
    "        \"en\": s,\n",
    "        \"greedy\": pred_greedy,\n",
    "        \"beam\": pred_beam\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHÂN TÍCH LỖI PHỔ BIẾN:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. Từ hiếm (OOV) → <unk>:\n",
    "   - Từ không có trong vocab 10k → mất thông tin cụ thể\n",
    "   - Giải pháp: Tăng vocab size, dùng BPE/subword tokenization\n",
    "\n",
    "2. Câu dài → mất thông tin (do context vector cố định):\n",
    "   - Encoder chỉ truyền (h_n, c_n) → thông tin bị nén\n",
    "   - Giải pháp: Đã dùng attention (Luong) để tập trung vào phần liên quan\n",
    "\n",
    "3. Dịch sai ngữ pháp, thiếu từ:\n",
    "   - Model có thể bỏ qua một số từ hoặc sai thứ tự\n",
    "   - Giải pháp: Beam search (đã implement) giúp tìm chuỗi tốt hơn\n",
    "\n",
    "4. Overfitting:\n",
    "   - Train loss giảm nhưng val loss chững lại sau một số epoch\n",
    "   - Giải pháp hiện tại: Dropout 0.3, early stopping (patience=3)\n",
    "   - Đề xuất cải tiến: Tăng dropout lên 0.4-0.5, thêm label smoothing\n",
    "\n",
    "5. Exposure bias:\n",
    "   - Training dùng ground truth (teacher forcing 0.5), inference dùng prediction → mismatch\n",
    "   - Giải pháp hiện tại: Teacher forcing 0.5 cố định (theo đề bài)\n",
    "   - Đề xuất cải tiến: Scheduled teacher forcing (giảm dần từ 0.7 → 0.3)\n",
    "\"\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
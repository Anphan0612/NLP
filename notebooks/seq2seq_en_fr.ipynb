{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anphan0612/NLP/blob/main/notebooks/seq2seq_en_fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brv4aBagaX0V"
      },
      "source": [
        "# Seq2Seq LSTM English→French (Multi30K)\n",
        "\n",
        "- Encoder-Decoder LSTM with fixed context vector, teacher forcing, greedy decoding.\n",
        "- Optional Luong attention + beam search for extended experiments.\n",
        "- Dataset: Multi30K en-fr raw (train/val/test) downloaded from GitHub.\n",
        "- Outputs: `best_model.pth`, `translate(sentence)`, BLEU score, loss plots, 5 example translations + error analysis notes.\n",
        "\n",
        "> Run this notebook end-to-end on Colab GPU (Python 3, torch ≥1.13).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7NKsH7maX0W",
        "outputId": "42b39986-17c2-4875-8423-9d7f2e4bd413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.1/183.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cài gói\n",
        "%pip install -q torch==2.4.1 torchtext==0.18.0 spacy==3.7.4 nltk==3.9.1 tqdm matplotlib\n",
        "%pip install -q sentencepiece  # tùy chọn nếu cần BPE sau này\n",
        "\n",
        "import spacy, nltk\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "spacy.cli.download(\"fr_core_news_sm\")\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SgLC6M9aX0X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRwsH3UbaX0X"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "DATA_DIR = Path(\"data/multi30k_en_fr\")\n",
        "RAW_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
        "for p in [RAW_DIR, PROCESSED_DIR, CHECKPOINT_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download Multi30K en-fr raw files if missing\n",
        "urls = {\n",
        "    \"train.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en\",\n",
        "    \"train.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.fr\",\n",
        "    \"val.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.en\",\n",
        "    \"val.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.fr\",\n",
        "    \"test.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.en\",\n",
        "    \"test.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.fr\",\n",
        "}\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file(url, dest):\n",
        "    if dest.exists():\n",
        "        return\n",
        "    print(f\"Downloading {dest.name} ...\")\n",
        "    r = requests.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    dest.write_bytes(r.content)\n",
        "\n",
        "for fname, url in urls.items():\n",
        "    download_file(url, RAW_DIR / fname)\n",
        "\n",
        "print(\"Data ready at\", RAW_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ_yRiwzaX0X"
      },
      "outputs": [],
      "source": [
        "# Load spaCy tokenizers\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "def tokenize_en(text: str) -> List[str]:\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text.strip())]\n",
        "\n",
        "\n",
        "def tokenize_fr(text: str) -> List[str]:\n",
        "    return [tok.text.lower() for tok in spacy_fr.tokenizer(text.strip())]\n",
        "\n",
        "SPECIALS = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8ZylT6maX0Y"
      },
      "outputs": [],
      "source": [
        "def read_parallel(split: str) -> Tuple[List[str], List[str]]:\n",
        "    en_path = RAW_DIR / f\"{split}.en\"\n",
        "    fr_path = RAW_DIR / f\"{split}.fr\"\n",
        "    with en_path.open(\"r\", encoding=\"utf-8\") as f_en, fr_path.open(\"r\", encoding=\"utf-8\") as f_fr:\n",
        "        en_lines = [line.strip() for line in f_en]\n",
        "        fr_lines = [line.strip() for line in f_fr]\n",
        "    assert len(en_lines) == len(fr_lines), \"Mismatched parallel data\"\n",
        "    return en_lines, fr_lines\n",
        "\n",
        "\n",
        "def yield_tokens(lines: List[str], tokenizer):\n",
        "    for line in lines:\n",
        "        yield tokenizer(line)\n",
        "\n",
        "\n",
        "def build_vocabs(max_size=10000, min_freq=2):\n",
        "    train_en, train_fr = read_parallel(\"train\")\n",
        "    en_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(train_en, tokenize_en),\n",
        "        specials=SPECIALS,\n",
        "        max_tokens=max_size,\n",
        "        min_freq=min_freq,\n",
        "    )\n",
        "    fr_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(train_fr, tokenize_fr),\n",
        "        specials=SPECIALS,\n",
        "        max_tokens=max_size,\n",
        "        min_freq=min_freq,\n",
        "    )\n",
        "    en_vocab.set_default_index(UNK_IDX)\n",
        "    fr_vocab.set_default_index(UNK_IDX)\n",
        "    return en_vocab, fr_vocab\n",
        "\n",
        "\n",
        "en_vocab, fr_vocab = build_vocabs()\n",
        "print(\"Vocab sizes:\", len(en_vocab), len(fr_vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0TRRSzdaX0Y"
      },
      "outputs": [],
      "source": [
        "class ParallelTextDataset(Dataset):\n",
        "    def __init__(self, split: str):\n",
        "        self.en_lines, self.fr_lines = read_parallel(split)\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en_lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_tok = tokenize_en(self.en_lines[idx])\n",
        "        fr_tok = tokenize_fr(self.fr_lines[idx])\n",
        "        en_ids = [SOS_IDX] + [en_vocab[t] for t in en_tok] + [EOS_IDX]\n",
        "        fr_ids = [SOS_IDX] + [fr_vocab[t] for t in fr_tok] + [EOS_IDX]\n",
        "        return torch.tensor(en_ids, dtype=torch.long), torch.tensor(fr_ids, dtype=torch.long)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    # sort by src length desc for packing\n",
        "    src_batch = list(src_batch)\n",
        "    tgt_batch = list(tgt_batch)\n",
        "    lengths = torch.tensor([len(x) for x in src_batch])\n",
        "    sorted_idx = torch.argsort(lengths, descending=True)\n",
        "    src_batch = [src_batch[i] for i in sorted_idx]\n",
        "    tgt_batch = [tgt_batch[i] for i in sorted_idx]\n",
        "    src_padded = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_padded = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_padded.to(device), tgt_padded.to(device), lengths[sorted_idx].to(device)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_ds = ParallelTextDataset(\"train\")\n",
        "val_ds = ParallelTextDataset(\"val\")\n",
        "test_ds = ParallelTextDataset(\"test\")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Batches:\", len(train_loader), len(val_loader), len(test_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hbyt9PSaX0Y"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src: [src_len, batch]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        packed = pack_padded_sequence(embedded, src_lengths.cpu(), enforce_sorted=True)\n",
        "        outputs, (hidden, cell) = self.rnn(packed)\n",
        "        outputs, _ = pad_packed_sequence(outputs)  # [src_len, batch, hid_dim * num_directions]\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hid_dim, hid_dim, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask=None):\n",
        "        # hidden: [batch, hid_dim]; encoder_outputs: [src_len, batch, hid_dim]\n",
        "        scores = torch.einsum(\"bh,sbh->bs\", self.attn(hidden), encoder_outputs)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_weights = torch.softmax(scores, dim=1)  # [batch, src_len]\n",
        "        context = torch.einsum(\"bs,sbh->bh\", attn_weights, encoder_outputs)\n",
        "        return context, attn_weights\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, use_attention=False):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim * (2 if use_attention else 1), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.use_attention = use_attention\n",
        "        self.attention = LuongAttention(hid_dim) if use_attention else None\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs=None, mask=None):\n",
        "        # input: [batch]; hidden/cell: [n_layers, batch, hid_dim]\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))  # [1, batch, emb_dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output: [1, batch, hid_dim]\n",
        "        output = output.squeeze(0)\n",
        "        if self.use_attention:\n",
        "            context, attn_weights = self.attention(output, encoder_outputs, mask)\n",
        "            output_cat = torch.cat((output, context), dim=1)\n",
        "            prediction = self.fc_out(output_cat)\n",
        "        else:\n",
        "            prediction = self.fc_out(output)\n",
        "            attn_weights = None\n",
        "        return prediction, hidden, cell, attn_weights\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx=PAD_IDX, use_attention=False):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "    def make_mask(self, src):\n",
        "        # src: [src_len, batch]\n",
        "        return (src != self.pad_idx).permute(1, 0)  # [batch, src_len]\n",
        "\n",
        "    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [src_len, batch]; trg: [trg_len, batch]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size, device=device)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
        "        input = trg[0, :]  # first token = <sos>\n",
        "        mask = self.make_mask(src) if self.use_attention else None\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fno0E0K3aX0Y"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters and model init\n",
        "EMB_DIM = 300\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "USE_ATTENTION = True  # set False to train baseline without attention\n",
        "BEAM_SIZE = 5\n",
        "MAX_LEN = 50\n",
        "TEACHER_FORCING = 0.5\n",
        "LR = 1e-3\n",
        "EPOCHS = 12\n",
        "PATIENCE = 3\n",
        "\n",
        "enc = Encoder(len(en_vocab), EMB_DIM, HID_DIM, n_layers=N_LAYERS, dropout=DROPOUT)\n",
        "dec = Decoder(len(fr_vocab), EMB_DIM, HID_DIM, n_layers=N_LAYERS, dropout=DROPOUT, use_attention=USE_ATTENTION)\n",
        "model = Seq2Seq(enc, dec, pad_idx=PAD_IDX, use_attention=USE_ATTENTION).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRk_kxFhaX0Y"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start, end):\n",
        "    elapsed = end - start\n",
        "    return int(elapsed // 60), int(elapsed % 60)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, teacher_forcing=0.5):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg, src_lengths in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, src_lengths, trg, teacher_forcing_ratio=teacher_forcing)\n",
        "        # outputs: [trg_len, batch, vocab]\n",
        "        output_dim = outputs.shape[-1]\n",
        "        outputs_flat = outputs[1:].reshape(-1, output_dim)\n",
        "        trg_flat = trg[1:].reshape(-1)\n",
        "        loss = criterion(outputs_flat, trg_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg, src_lengths in tqdm(loader, desc=\"Val\", leave=False):\n",
        "            outputs = model(src, src_lengths, trg, teacher_forcing_ratio=0.0)\n",
        "            output_dim = outputs.shape[-1]\n",
        "            outputs_flat = outputs[1:].reshape(-1, output_dim)\n",
        "            trg_flat = trg[1:].reshape(-1)\n",
        "            loss = criterion(outputs_flat, trg_flat)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQq3OCjtaX0Y"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "patience_counter = 0\n",
        "train_losses, val_losses = [], []\n",
        "best_path = CHECKPOINT_DIR / \"best_model.pth\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    start_time = time.time()\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, teacher_forcing=TEACHER_FORCING)\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    scheduler.step(val_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save({\"model_state\": model.state_dict(), \"config\": {\n",
        "            \"EMB_DIM\": EMB_DIM,\n",
        "            \"HID_DIM\": HID_DIM,\n",
        "            \"N_LAYERS\": N_LAYERS,\n",
        "            \"DROPOUT\": DROPOUT,\n",
        "            \"USE_ATTENTION\": USE_ATTENTION,\n",
        "            \"EN_VOCAB\": len(en_vocab),\n",
        "            \"FR_VOCAB\": len(fr_vocab),\n",
        "            \"PAD_IDX\": PAD_IDX,\n",
        "        }}, best_path)\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    mins, secs = epoch_time(start_time, time.time())\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train {train_loss:.3f} | Val {val_loss:.3f} | Time {mins}m {secs}s\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "print(\"Best checkpoint:\", best_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOqt-IzGaX0Y"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_losses, label=\"train\")\n",
        "plt.plot(val_losses, label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train/Val Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBUeDW9JaX0Y"
      },
      "outputs": [],
      "source": [
        "# Inference helpers\n",
        "\n",
        "def ids_to_sentence(ids, vocab):\n",
        "    tokens = []\n",
        "    for i in ids:\n",
        "        if i == EOS_IDX:\n",
        "            break\n",
        "        if i not in (SOS_IDX, PAD_IDX):\n",
        "            tokens.append(vocab.get_itos()[i])\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def encode_sentence(sentence, tokenizer, vocab):\n",
        "    toks = [t for t in tokenizer(sentence)]\n",
        "    ids = [SOS_IDX] + [vocab[t] for t in toks] + [EOS_IDX]\n",
        "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "def greedy_decode(model, sentence, max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_tensor = encode_sentence(sentence, tokenize_en, en_vocab).unsqueeze(1)\n",
        "        src_len = torch.tensor([src_tensor.shape[0]], device=device)\n",
        "        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)\n",
        "        mask = model.make_mask(src_tensor) if model.use_attention else None\n",
        "        input_token = torch.tensor([SOS_IDX], device=device)\n",
        "        preds = []\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell, _ = model.decoder(input_token, hidden, cell, encoder_outputs, mask)\n",
        "            top1 = output.argmax(1)\n",
        "            if top1.item() == EOS_IDX:\n",
        "                break\n",
        "            preds.append(top1.item())\n",
        "            input_token = top1\n",
        "        return ids_to_sentence(preds, fr_vocab)\n",
        "\n",
        "\n",
        "def beam_search_decode(model, sentence, beam_size=5, max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_tensor = encode_sentence(sentence, tokenize_en, en_vocab).unsqueeze(1)\n",
        "        src_len = torch.tensor([src_tensor.shape[0]], device=device)\n",
        "        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)\n",
        "        mask = model.make_mask(src_tensor) if model.use_attention else None\n",
        "\n",
        "        beams = [(0.0, [SOS_IDX], hidden, cell)]\n",
        "        completed = []\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "            for log_prob, seq, h, c in beams:\n",
        "                inp = torch.tensor([seq[-1]], device=device)\n",
        "                output, h_new, c_new, _ = model.decoder(inp, h, c, encoder_outputs, mask)\n",
        "                probs = torch.log_softmax(output, dim=1)\n",
        "                topk_logp, topk_idx = probs.topk(beam_size, dim=1)\n",
        "                for k in range(beam_size):\n",
        "                    next_token = topk_idx[0, k].item()\n",
        "                    new_log_prob = log_prob + topk_logp[0, k].item()\n",
        "                    new_seq = seq + [next_token]\n",
        "                    if next_token == EOS_IDX:\n",
        "                        completed.append((new_log_prob, new_seq))\n",
        "                    else:\n",
        "                        new_beams.append((new_log_prob, new_seq, h_new, c_new))\n",
        "            beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_size]\n",
        "            if not beams:\n",
        "                break\n",
        "        if completed:\n",
        "            best = max(completed, key=lambda x: x[0])\n",
        "        else:\n",
        "            best = max(beams, key=lambda x: x[0])\n",
        "        return ids_to_sentence(best[1], fr_vocab)\n",
        "\n",
        "\n",
        "def translate(sentence, use_beam=False):\n",
        "    if use_beam:\n",
        "        return beam_search_decode(model, sentence, beam_size=BEAM_SIZE, max_len=MAX_LEN)\n",
        "    return greedy_decode(model, sentence, max_len=MAX_LEN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDDvGyLQaX0Y"
      },
      "outputs": [],
      "source": [
        "# BLEU evaluation on test set\n",
        "\n",
        "def compute_bleu(model, loader, n_samples=None, use_beam=False):\n",
        "    model.eval()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    scores = []\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg, src_lengths) in enumerate(tqdm(loader, desc=\"Test\", leave=False)):\n",
        "            # decode each sentence individually for simplicity\n",
        "            for b in range(src.shape[1]):\n",
        "                src_sent = src[:, b].tolist()\n",
        "                trg_sent = trg[:, b].tolist()\n",
        "                # reconstruct raw English sentence for translation\n",
        "                en_tokens = [en_vocab.get_itos()[idx] for idx in src_sent if idx not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
        "                src_text = \" \".join(en_tokens)\n",
        "                pred = translate(src_text, use_beam=use_beam)\n",
        "                ref_tokens = [fr_vocab.get_itos()[idx] for idx in trg_sent if idx not in (PAD_IDX, SOS_IDX, EOS_IDX, EOS_IDX)]\n",
        "                scores.append(sentence_bleu([ref_tokens], pred.split(), smoothing_function=smoothie))\n",
        "            if n_samples and len(scores) >= n_samples:\n",
        "                break\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "# Compute BLEU (may take time). Uncomment to run after training.\n",
        "# bleu_score = compute_bleu(model, test_loader, n_samples=200, use_beam=False)\n",
        "# print(\"BLEU (greedy):\", bleu_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAoRJOf3aX0Y"
      },
      "outputs": [],
      "source": [
        "# Quick demo translations (run after training/loading checkpoint)\n",
        "sentences = [\n",
        "    \"a man is riding a bicycle\",\n",
        "    \"children are playing in the park\",\n",
        "    \"a woman is sitting at a table\",\n",
        "    \"two dogs are running on the beach\",\n",
        "    \"the man holds a red umbrella\",\n",
        "]\n",
        "\n",
        "# for s in sentences:\n",
        "#     print(\"EN:\", s)\n",
        "#     print(\"FR (greedy):\", translate(s, use_beam=False))\n",
        "#     print(\"FR (beam):\", translate(s, use_beam=True))\n",
        "#     print(\"---\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}